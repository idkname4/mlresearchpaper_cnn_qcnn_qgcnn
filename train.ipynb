{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c1432c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Phase1: UCF101 structure & annotation quick report ===\n",
      "Found 1 class folders, total videos = 0\n",
      "Top 8 classes by video count:\n",
      "  UCF-101: 0 files\n",
      "\n",
      "Sample files (first few per class):\n",
      "  UCF-101: []\n",
      "\n",
      "=== Load annotation files and cross-check listed video paths ===\n",
      "Found annotation files: ['classInd.txt', 'testlist01.txt', 'testlist02.txt', 'testlist03.txt', 'trainlist01.txt', 'trainlist02.txt', 'trainlist03.txt']\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c06.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g02_c01.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g02_c02.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g02_c03.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g02_c04.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c01.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c02.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c03.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c04.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c05.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c06.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g04_c01.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g04_c02.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g04_c03.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g04_c04.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g04_c05.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g04_c06.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g04_c07.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c01.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c02.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c03.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c04.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c05.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c06.avi\n",
      "[MISSING] testlist01.txt: ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c07.avi\n",
      "Checked 39960 annotation entries; missing references: 39960\n",
      "...and 39930 more missing entries (truncated report).\n",
      "\n",
      "=== Quick readability test of video files (first frame) ===\n",
      "Checked 0 videos; unreadable/corrupt found: 0\n",
      "\n",
      "Phase 1 report saved -> phase1_report.json\n",
      "Summary: {'classes_count': 1, 'readability_checked': 0, 'readability_unreadable_count': 0, 'missing_annotation_references_count': 39960}\n"
     ]
    }
   ],
   "source": [
    "# Phase 1 — Data validation & optional cleaning for UCF101\n",
    "# Paste this whole cell into train.ipynb and run.\n",
    "# It will:\n",
    "#  - Report class-folder counts and total videos\n",
    "#  - Verify annotation files (train/test/classInd) and cross-check listed video paths\n",
    "#  - Check readability of video files (first frame) for a sample or full scan and optionally move corrupt files\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import json\n",
    "import shutil\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------- CONFIG (change paths if needed) --------\n",
    "UCF_ROOT = Path(\"UCF101\")   # folder with 101 class subfolders of .avi videos\n",
    "ANN_DIR  = Path(\"UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist\")  # folder with trainlist/testlist/classInd\n",
    "MOVE_CORRUPT = False        # set True to move unreadable files to ./_corrupt_videos\n",
    "MAX_CHECK_VIDEOS = 500      # set None to scan all videos (can be slow); useful to limit for a quick run\n",
    "SAMPLE_PER_CLASS = 3        # print a few samples per class for verification\n",
    "# -------------------------------------------------\n",
    "\n",
    "def is_video_file(p: Path):\n",
    "    return p.is_file() and p.suffix.lower() in {\".avi\", \".mp4\", \".mov\", \".mkv\"}\n",
    "\n",
    "def report_structure(root: Path, ann_dir: Path):\n",
    "    assert root.exists(), f\"UCF root not found: {root}\"\n",
    "    assert ann_dir.exists(), f\"Annotation folder not found: {ann_dir}\"\n",
    "    classes = sorted([d.name for d in root.iterdir() if d.is_dir()])\n",
    "    class_counts = {}\n",
    "    total_videos = 0\n",
    "    samples = {}\n",
    "    for c in classes:\n",
    "        folder = root / c\n",
    "        vids = [f for f in folder.iterdir() if is_video_file(f)]\n",
    "        class_counts[c] = len(vids)\n",
    "        total_videos += len(vids)\n",
    "        samples[c] = [str(v.name) for v in vids[:SAMPLE_PER_CLASS]]\n",
    "    print(f\"Found {len(classes)} class folders, total videos = {total_videos}\")\n",
    "    print(\"Top 8 classes by video count:\")\n",
    "    for cls, cnt in Counter(class_counts).most_common(8):\n",
    "        print(f\"  {cls}: {class_counts[cls]} files\")\n",
    "    print(\"\\nSample files (first few per class):\")\n",
    "    for c in list(samples.keys())[:6]:\n",
    "        print(f\"  {c}: {samples[c]}\")\n",
    "    return classes, class_counts\n",
    "\n",
    "def load_annotation_lists(ann_dir: Path):\n",
    "    # read classInd.txt, trainlist/testlist*.txt (safe parsing)\n",
    "    ann_files = list(sorted(ann_dir.glob(\"*.txt\")))\n",
    "    ann_data = {}\n",
    "    for f in ann_files:\n",
    "        key = f.name\n",
    "        with open(f, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "            lines = [ln.strip() for ln in fh if ln.strip()]\n",
    "        ann_data[key] = lines\n",
    "    print(f\"Found annotation files: {sorted(ann_data.keys())}\")\n",
    "    # return dictionary of filename -> lines\n",
    "    return ann_data\n",
    "\n",
    "def crosscheck_annotations(root: Path, ann_data: dict, limit_report=20):\n",
    "    # Many train/test lists contain entries like: ClassName/v_ClassName_g01_c01.avi [label]\n",
    "    # We'll parse and verify the existence of each referenced video path under root.\n",
    "    missing = []\n",
    "    total_ref = 0\n",
    "    for fname, lines in ann_data.items():\n",
    "        # focus on train/test lists and classInd separately\n",
    "        if fname.lower().startswith(\"classind\"):\n",
    "            # classInd lines expected: \"<index> <ClassName>\"\n",
    "            continue\n",
    "        for ln in lines:\n",
    "            total_ref += 1\n",
    "            # split by whitespace and take first token as path\n",
    "            token = ln.split()[0]\n",
    "            candidate = root / token\n",
    "            if not candidate.exists():\n",
    "                missing.append((fname, token))\n",
    "                if len(missing) <= limit_report:\n",
    "                    print(f\"[MISSING] {fname}: {token}\")\n",
    "    print(f\"Checked {total_ref} annotation entries; missing references: {len(missing)}\")\n",
    "    if len(missing) and len(missing) > limit_report:\n",
    "        print(f\"...and {len(missing)-limit_report} more missing entries (truncated report).\")\n",
    "    return missing\n",
    "\n",
    "def check_videos_readable(root: Path, max_videos=None, move_corrupt=False):\n",
    "    moved = []\n",
    "    checked = 0\n",
    "    corrupt_dir = root.parent / \"_corrupt_videos\"\n",
    "    if move_corrupt:\n",
    "        corrupt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for cls in sorted([d for d in root.iterdir() if d.is_dir()]):\n",
    "        for vid in cls.iterdir():\n",
    "            if not is_video_file(vid):\n",
    "                continue\n",
    "            # optional early stop\n",
    "            if max_videos and checked >= max_videos:\n",
    "                return {\"checked\": checked, \"moved\": moved}\n",
    "            checked += 1\n",
    "            try:\n",
    "                cap = cv2.VideoCapture(str(vid))\n",
    "                ok, frame = cap.read()\n",
    "                cap.release()\n",
    "                if not ok or frame is None:\n",
    "                    # treat as corrupt/unreadable\n",
    "                    if move_corrupt:\n",
    "                        target_folder = corrupt_dir / cls.name\n",
    "                        target_folder.mkdir(parents=True, exist_ok=True)\n",
    "                        shutil.move(str(vid), str(target_folder / vid.name))\n",
    "                        moved.append(str(vid))\n",
    "                    else:\n",
    "                        moved.append(str(vid))\n",
    "            except Exception as e:\n",
    "                moved.append(str(vid))\n",
    "    print(f\"Checked {checked} videos; unreadable/corrupt found: {len(moved)}\")\n",
    "    if move_corrupt:\n",
    "        print(f\"Moved corrupt files to: {corrupt_dir}\")\n",
    "    return {\"checked\": checked, \"moved\": moved}\n",
    "\n",
    "# -------- Run Phase 1 checks --------\n",
    "print(\"=== Phase1: UCF101 structure & annotation quick report ===\")\n",
    "classes, class_counts = report_structure(UCF_ROOT, ANN_DIR)\n",
    "\n",
    "print(\"\\n=== Load annotation files and cross-check listed video paths ===\")\n",
    "ann_data = load_annotation_lists(ANN_DIR)\n",
    "missing_refs = crosscheck_annotations(UCF_ROOT, ann_data, limit_report=30)\n",
    "\n",
    "print(\"\\n=== Quick readability test of video files (first frame) ===\")\n",
    "# For speed we do a limited check by default; set MAX_CHECK_VIDEOS=None to scan all.\n",
    "readability_report = check_videos_readable(UCF_ROOT, max_videos=MAX_CHECK_VIDEOS, move_corrupt=MOVE_CORRUPT)\n",
    "\n",
    "# Save a small JSON report you can inspect later\n",
    "report = {\n",
    "    \"classes_count\": len(classes),\n",
    "    \"per_class_counts\": class_counts,\n",
    "    \"annotations_files\": sorted(list(ann_data.keys())),\n",
    "    \"missing_annotation_references_count\": len(missing_refs),\n",
    "    \"sample_missing_refs\": missing_refs[:10],\n",
    "    \"readability_checked\": readability_report[\"checked\"],\n",
    "    \"readability_unreadable_count\": len(readability_report[\"moved\"]),\n",
    "    \"moved_files_if_any\": readability_report[\"moved\"][:20],\n",
    "}\n",
    "with open(\"phase1_report.json\", \"w\") as fh:\n",
    "    json.dump(report, fh, indent=2)\n",
    "\n",
    "print(\"\\nPhase 1 report saved -> phase1_report.json\")\n",
    "print(\"Summary:\", {k: report[k] for k in [\"classes_count\", \"readability_checked\", \"readability_unreadable_count\", \"missing_annotation_references_count\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c5da278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE: C:\\Users\\HP\\mlresearchpaper\\UCF101\n",
      "Top-level children under BASE (name : type):\n",
      " - UCF-101  (dir)  subdirs=101  immediate_vids=0\n",
      "\n",
      "Auto-detected candidate inner folder that looks like dataset root:\n",
      " -> UCF101/UCF-101  | classes_with_vids=101  | total_vids_in_these=13320\n",
      "\n",
      "Using dataset root = UCF101\\UCF-101  | class folders found = 101\n",
      "Sample class folder video counts (first 8):\n",
      " - ApplyEyeMakeup: 145\n",
      " - ApplyLipstick: 114\n",
      " - Archery: 145\n",
      " - BabyCrawling: 132\n",
      " - BalanceBeam: 108\n",
      " - BandMarching: 155\n",
      " - BaseballPitch: 150\n",
      " - Basketball: 134\n",
      "\n",
      "classInd entries found: 101  | folder names: 101\n",
      "Examples (classInd first 6): ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching']\n",
      "Examples (folders first 6): ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching']\n",
      "\n",
      "classInd names missing as folders (count=0): []\n",
      "Extra folders not in classInd (count=0): []\n",
      "\n",
      "Wrote helper report -> phase1_root_autodetect.json\n",
      "If SUGGESTED_ROOT looks correct, set UCF_ROOT = Path('<that path>') in the next cell and re-run the Phase1 checks.\n"
     ]
    }
   ],
   "source": [
    "# Fix dataset-root detection + compare classInd -> folder names\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "BASE = Path(\"UCF101\")   # current folder you used\n",
    "ANN_DIR = Path(\"UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist\")\n",
    "CLASSIND_PATH = ANN_DIR / \"classInd.txt\"\n",
    "\n",
    "def count_video_files(p):\n",
    "    exts = {\".avi\", \".mp4\", \".mkv\", \".mov\"}\n",
    "    files = [f for f in p.rglob(\"*\") if f.suffix.lower() in exts]\n",
    "    return len(files)\n",
    "\n",
    "# 1) Inspect top-level children under BASE\n",
    "print(\"BASE:\", BASE.resolve())\n",
    "children = sorted([c for c in BASE.iterdir()]) if BASE.exists() else []\n",
    "print(\"Top-level children under BASE (name : type):\")\n",
    "for c in children[:30]:\n",
    "    t = \"dir\" if c.is_dir() else \"file\"\n",
    "    # if dir, count immediate subdirs & few video files if present\n",
    "    subdirs = len([d for d in c.iterdir() if d.is_dir()]) if c.is_dir() else 0\n",
    "    vids_immediate = len([f for f in c.iterdir() if f.is_file() and f.suffix.lower() in {'.avi','.mp4'}]) if c.is_dir() else 0\n",
    "    print(f\" - {c.name}  ({t})  subdirs={subdirs}  immediate_vids={vids_immediate}\")\n",
    "\n",
    "# 2) Try to auto-detect the real UCF root: a folder that contains many class subfolders\n",
    "candidate = None\n",
    "best_score = -1\n",
    "for c in children:\n",
    "    if not c.is_dir(): \n",
    "        continue\n",
    "    # score by number of subdirs that themselves contain video files\n",
    "    subdirs = [d for d in c.iterdir() if d.is_dir()]\n",
    "    count_subdirs_with_vids = 0\n",
    "    total_vids = 0\n",
    "    for sd in subdirs:\n",
    "        vids = [f for f in sd.iterdir() if f.is_file() and f.suffix.lower() in {'.avi','.mp4','.mkv','.mov'}]\n",
    "        if vids:\n",
    "            count_subdirs_with_vids += 1\n",
    "            total_vids += len(vids)\n",
    "    score = count_subdirs_with_vids + (total_vids/1000.0)  # heuristics\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        candidate = (c, count_subdirs_with_vids, total_vids)\n",
    "\n",
    "if candidate:\n",
    "    print(\"\\nAuto-detected candidate inner folder that looks like dataset root:\")\n",
    "    print(f\" -> {candidate[0].as_posix()}  | classes_with_vids={candidate[1]}  | total_vids_in_these={candidate[2]}\")\n",
    "    SUGGESTED_ROOT = candidate[0]\n",
    "else:\n",
    "    # if nothing, maybe BASE itself has class folders\n",
    "    print(\"\\nNo inner candidate found; falling back to BASE as root\")\n",
    "    SUGGESTED_ROOT = BASE\n",
    "\n",
    "# 3) Quick re-check of SUGGESTED_ROOT: list first 8 class folders and counts\n",
    "sr = SUGGESTED_ROOT\n",
    "if not sr.exists():\n",
    "    raise FileNotFoundError(f\"Suggested root does not exist: {sr}\")\n",
    "class_dirs = sorted([d for d in sr.iterdir() if d.is_dir()])\n",
    "print(f\"\\nUsing dataset root = {sr}  | class folders found = {len(class_dirs)}\")\n",
    "sample = []\n",
    "for d in class_dirs[:8]:\n",
    "    vids = [f for f in d.iterdir() if f.is_file() and f.suffix.lower() in {'.avi','.mp4','.mkv','.mov'}]\n",
    "    sample.append((d.name, len(vids)))\n",
    "print(\"Sample class folder video counts (first 8):\")\n",
    "for name,cnt in sample:\n",
    "    print(f\" - {name}: {cnt}\")\n",
    "\n",
    "# 4) Compare classInd.txt names to folder names (exact match)\n",
    "if CLASSIND_PATH.exists():\n",
    "    with open(CLASSIND_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "        classind_lines = [ln.strip() for ln in fh if ln.strip()]\n",
    "    # classInd lines typically: \"<index> <ClassName>\"\n",
    "    class_names_from_file = [ln.split()[-1] for ln in classind_lines]\n",
    "    folder_names = [d.name for d in class_dirs]\n",
    "    set_file = set(class_names_from_file)\n",
    "    set_folders = set(folder_names)\n",
    "    missing_in_fs = sorted(list(set_file - set_folders))\n",
    "    extra_folders = sorted(list(set_folders - set_file))\n",
    "    print(f\"\\nclassInd entries found: {len(class_names_from_file)}  | folder names: {len(folder_names)}\")\n",
    "    print(\"Examples (classInd first 6):\", class_names_from_file[:6])\n",
    "    print(\"Examples (folders first 6):\", folder_names[:6])\n",
    "    print(f\"\\nclassInd names missing as folders (count={len(missing_in_fs)}): {missing_in_fs[:10]}\")\n",
    "    print(f\"Extra folders not in classInd (count={len(extra_folders)}): {extra_folders[:10]}\")\n",
    "else:\n",
    "    print(\"\\nclassInd.txt not found at expected path:\", CLASSIND_PATH)\n",
    "\n",
    "# Save a small helper file so you can inspect results\n",
    "out = {\n",
    "    \"base\": str(BASE),\n",
    "    \"suggested_root\": str(SUGGESTED_ROOT),\n",
    "    \"detected_classes_count\": len(class_dirs),\n",
    "    \"sample_class_counts\": sample,\n",
    "    \"classind_missing_count\": len(missing_in_fs) if CLASSIND_PATH.exists() else None,\n",
    "    \"extra_folders_count\": len(extra_folders) if CLASSIND_PATH.exists() else None\n",
    "}\n",
    "with open(\"phase1_root_autodetect.json\",\"w\") as fh:\n",
    "    json.dump(out, fh, indent=2)\n",
    "\n",
    "print(\"\\nWrote helper report -> phase1_root_autodetect.json\")\n",
    "print(\"If SUGGESTED_ROOT looks correct, set UCF_ROOT = Path('<that path>') in the next cell and re-run the Phase1 checks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b1ce20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using root: C:\\Users\\HP\\mlresearchpaper\\UCF101\\UCF-101\n",
      "Detected class folders: 101, total videos (approx): 13320\n",
      "Sample first 8 classes and counts:\n",
      " - ApplyEyeMakeup: 145 files\n",
      " - ApplyLipstick: 114 files\n",
      " - Archery: 145 files\n",
      " - BabyCrawling: 132 files\n",
      " - BalanceBeam: 108 files\n",
      " - BandMarching: 155 files\n",
      " - BaseballPitch: 150 files\n",
      " - Basketball: 134 files\n",
      "classInd entries: 101 (first 6): ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching']\n",
      "Folder vs classInd exact-match check: OK\n",
      "Readability check: checked=500, unreadable_found=0\n",
      "Wrote phase1_report_corrected.json\n"
     ]
    }
   ],
   "source": [
    "# Fix root and re-run Phase-1 quick checks (paste & run)\n",
    "from pathlib import Path\n",
    "import json, cv2, shutil\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- set corrected root here ----\n",
    "UCF_ROOT = Path(\"UCF101\") / \"UCF-101\"      # <- CORRECTED root\n",
    "ANN_DIR  = Path(\"UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist\")\n",
    "MOVE_CORRUPT = False\n",
    "MAX_CHECK_VIDEOS = 500\n",
    "\n",
    "def is_video_file(p): return p.is_file() and p.suffix.lower() in {'.avi','.mp4','.mkv','.mov'}\n",
    "\n",
    "# quick counts\n",
    "class_dirs = sorted([d for d in UCF_ROOT.iterdir() if d.is_dir()])\n",
    "total_videos = sum(len([f for f in d.iterdir() if is_video_file(f)]) for d in class_dirs)\n",
    "print(f\"Using root: {UCF_ROOT.resolve()}\")\n",
    "print(f\"Detected class folders: {len(class_dirs)}, total videos (approx): {total_videos}\")\n",
    "print(\"Sample first 8 classes and counts:\")\n",
    "for d in class_dirs[:8]:\n",
    "    print(f\" - {d.name}: {len([f for f in d.iterdir() if is_video_file(f)])} files\")\n",
    "\n",
    "# verify annotations exist & basic cross-check (small sample)\n",
    "classind = ANN_DIR / \"classInd.txt\"\n",
    "if classind.exists():\n",
    "    with open(classind, 'r', errors='ignore') as fh:\n",
    "        classind_names = [ln.strip().split()[-1] for ln in fh if ln.strip()]\n",
    "    print(f\"classInd entries: {len(classind_names)} (first 6): {classind_names[:6]}\")\n",
    "    print(\"Folder vs classInd exact-match check:\", \"OK\" if set(classind_names)==set([d.name for d in class_dirs]) else \"MISMATCH - inspect\")\n",
    "else:\n",
    "    print(\"Warning: classInd.txt not found at\", classind)\n",
    "\n",
    "# lightweight readability check (first MAX_CHECK_VIDEOS videos)\n",
    "checked = 0\n",
    "unreadable = []\n",
    "for cls in class_dirs:\n",
    "    for vid in cls.iterdir():\n",
    "        if not is_video_file(vid): continue\n",
    "        if MAX_CHECK_VIDEOS and checked >= MAX_CHECK_VIDEOS: break\n",
    "        checked += 1\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(str(vid)); ok, fr = cap.read(); cap.release()\n",
    "            if not ok or fr is None:\n",
    "                unreadable.append(str(vid))\n",
    "        except:\n",
    "            unreadable.append(str(vid))\n",
    "    if MAX_CHECK_VIDEOS and checked >= MAX_CHECK_VIDEOS: break\n",
    "\n",
    "print(f\"Readability check: checked={checked}, unreadable_found={len(unreadable)}\")\n",
    "out = {\n",
    "    \"corrected_root\": str(UCF_ROOT),\n",
    "    \"detected_class_folders\": len(class_dirs),\n",
    "    \"total_videos_approx\": total_videos,\n",
    "    \"readability_checked\": checked,\n",
    "    \"readability_unreadable_count\": len(unreadable),\n",
    "    \"sample_unreadable\": unreadable[:10]\n",
    "}\n",
    "with open(\"phase1_report_corrected.json\", \"w\") as fh:\n",
    "    json.dump(out, fh, indent=2)\n",
    "print(\"Wrote phase1_report_corrected.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0313f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classes: 100%|██████████| 101/101 [08:39<00:00,  5.14s/it]\n",
      "Flatten classes: 100%|██████████| 101/101 [04:11<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame extraction complete.\n",
      "Classes processed: 101\n",
      "Total frames extracted (approx): 37703\n",
      "Frames stored under: C:\\Users\\HP\\mlresearchpaper\\frames\n",
      "Flattened ImageFolder ready at: C:\\Users\\HP\\mlresearchpaper\\frames_flat\n",
      "Wrote phase2_frames_report.json\n"
     ]
    }
   ],
   "source": [
    "# Phase 2 — Frame extraction + optional flattened dev subset\n",
    "# Paste & run this whole cell in train.ipynb\n",
    "# Outputs: frames_root/<class>/<video_id>/frame_000001.jpg ... \n",
    "# Optional flattened shortcut: frames_flat/<class>/frame_<videoid>_<fnum>.jpg for quick ImageFolder use.\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "# ----- CONFIG (edit as needed) -----\n",
    "DATA_ROOT = Path(\"UCF101\") / \"UCF-101\"          # corrected dataset root (from Phase1)\n",
    "OUT_FRAMES_ROOT = Path(\"frames\")                # where frames will be stored\n",
    "FRAME_RATE = 10                                 # save every Nth frame (1 => every frame)\n",
    "RESIZE = (112, 112)                             # (W,H) resize for all frames; set None to keep original\n",
    "CLASSES_LIMIT = None                             # e.g. [\"Basketball\",\"Biking\"] or None to process all 101\n",
    "MAX_VIDEOS_PER_CLASS = 20                       # set small number for dev subset; None for all\n",
    "SKIP_IF_EXISTS = True                           # skip extraction if target folder already exists\n",
    "CREATE_FLAT = True                              # also create frames_flat/<class>/ for ImageFolder experiments\n",
    "FLAT_PER_CLASS_LIMIT = 500                      # maximum flattened frames per class (across all videos)\n",
    "# ------------------------------------\n",
    "\n",
    "def is_video_file(p: Path):\n",
    "    return p.is_file() and p.suffix.lower() in {\".avi\", \".mp4\", \".mkv\", \".mov\"}\n",
    "\n",
    "def extract_frames_from_video(video_path: Path, out_dir: Path, frame_rate:int=10, resize=None):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    count = 0\n",
    "    saved = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % frame_rate == 0:\n",
    "            if resize:\n",
    "                frame = cv2.resize(frame, resize)\n",
    "            out_file = out_dir / f\"frame_{saved:06d}.jpg\"\n",
    "            cv2.imwrite(str(out_file), frame)\n",
    "            saved += 1\n",
    "        count += 1\n",
    "    cap.release()\n",
    "    return saved\n",
    "\n",
    "# ---- main loop ----\n",
    "DATA_ROOT = Path(DATA_ROOT)\n",
    "OUT_FRAMES_ROOT = Path(OUT_FRAMES_ROOT)\n",
    "OUT_FRAMES_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "class_dirs = sorted([d for d in DATA_ROOT.iterdir() if d.is_dir()])\n",
    "if CLASSES_LIMIT:\n",
    "    class_dirs = [d for d in class_dirs if d.name in CLASSES_LIMIT]\n",
    "\n",
    "summary = {}\n",
    "for cls in tqdm(class_dirs, desc=\"Classes\"):\n",
    "    vids = [v for v in cls.iterdir() if is_video_file(v)]\n",
    "    vids = sorted(vids)\n",
    "    if MAX_VIDEOS_PER_CLASS:\n",
    "        vids = vids[:MAX_VIDEOS_PER_CLASS]\n",
    "    class_out = OUT_FRAMES_ROOT / cls.name\n",
    "    extracted_for_class = 0\n",
    "    for vid in tqdm(vids, desc=f\"  {cls.name}\", leave=False):\n",
    "        video_id = vid.stem\n",
    "        target_dir = class_out / video_id\n",
    "        if SKIP_IF_EXISTS and target_dir.exists() and any(target_dir.iterdir()):\n",
    "            # assume already extracted\n",
    "            extracted = len(list(target_dir.glob(\"*.jpg\")))\n",
    "            extracted_for_class += extracted\n",
    "            continue\n",
    "        try:\n",
    "            extracted = extract_frames_from_video(vid, target_dir, frame_rate=FRAME_RATE, resize=RESIZE)\n",
    "            extracted_for_class += extracted\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting {vid}: {e}\")\n",
    "    summary[cls.name] = extracted_for_class\n",
    "\n",
    "# ---- optional: create flattened ImageFolder-like structure ----\n",
    "if CREATE_FLAT:\n",
    "    FLAT_ROOT = Path(\"frames_flat\")\n",
    "    if FLAT_ROOT.exists():\n",
    "        # do not delete by default; warn and reuse\n",
    "        pass\n",
    "    FLAT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "    for cls, num_ex in tqdm(summary.items(), desc=\"Flatten classes\"):\n",
    "        flat_class_dir = FLAT_ROOT / cls\n",
    "        flat_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "        # iterate through video subfolders and copy frames until limit reached\n",
    "        copied = 0\n",
    "        video_dirs = sorted((OUT_FRAMES_ROOT / cls).iterdir()) if (OUT_FRAMES_ROOT/cls).exists() else []\n",
    "        for vd in video_dirs:\n",
    "            for f in sorted(vd.glob(\"*.jpg\")):\n",
    "                if copied >= FLAT_PER_CLASS_LIMIT:\n",
    "                    break\n",
    "                dst = flat_class_dir / f\"{vd.name}_{f.name}\"\n",
    "                if not dst.exists():\n",
    "                    shutil.copy2(str(f), str(dst))\n",
    "                    copied += 1\n",
    "            if copied >= FLAT_PER_CLASS_LIMIT:\n",
    "                break\n",
    "\n",
    "# ---- summary output ----\n",
    "total_frames = sum(summary.values())\n",
    "total_videos = sum(1 for c in class_dirs for _ in (c.iterdir() if c.exists() else []) if is_video_file(_))\n",
    "print(\"Frame extraction complete.\")\n",
    "print(f\"Classes processed: {len(summary)}\")\n",
    "print(f\"Total frames extracted (approx): {total_frames}\")\n",
    "print(f\"Frames stored under: {OUT_FRAMES_ROOT.resolve()}\")\n",
    "if CREATE_FLAT:\n",
    "    print(f\"Flattened ImageFolder ready at: {Path('frames_flat').resolve()}\")\n",
    "# write tiny report\n",
    "import json\n",
    "with open(\"phase2_frames_report.json\",\"w\") as fh:\n",
    "    json.dump({\"classes_processed\": len(summary), \"per_class_frames\": {k:int(v) for k,v in summary.items()}, \"total_frames\": int(total_frames)}, fh, indent=2)\n",
    "print(\"Wrote phase2_frames_report.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64eb5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Found classes=101, total_images=35375, train=28300, val=7075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\HP/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44.7M/44.7M [00:09<00:00, 4.97MB/s]\n",
      "Train Batches:   0%|          | 0/885 [00:00<?, ?it/s]c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                                                                    \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 118\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, NUM_EPOCHS+\u001b[32m1\u001b[39m):\n\u001b[32m    117\u001b[39m     t0 = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     train_loss, train_acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m     val_loss, val_acc = validate(model, val_loader, criterion, device)\n\u001b[32m    120\u001b[39m     scheduler.step()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, optimizer, criterion, device)\u001b[39m\n\u001b[32m     85\u001b[39m outputs = model(inputs)\n\u001b[32m     86\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m optimizer.step()\n\u001b[32m     89\u001b[39m losses.append(loss.item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Phase 3 — Baseline CNN training (ResNet18 fine-tune)\n",
    "# Paste & run in train.ipynb. Adjust CONFIG below if needed.\n",
    "\n",
    "import os, json, time\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------- CONFIG ---------------\n",
    "FRAMES_FLAT = Path(\"frames_flat\")           # imagefolder root made in Phase2\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 6\n",
    "LR = 1e-4\n",
    "NUM_WORKERS = 4\n",
    "IMG_SIZE = 224                              # resize for ResNet\n",
    "CHECKPOINT_PATH = \"cnn_resnet18_best.pth\"\n",
    "REPORT_PATH = \"phase3_train_report.json\"\n",
    "USE_PRETRAINED = True                       # fine-tune pretrained resnet18\n",
    "PIN_MEMORY = True\n",
    "# --------------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---- transforms & dataloaders ----\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(int(IMG_SIZE*1.15)),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# split ImageFolder into train/val by folder-split (80/20)\n",
    "dataset = datasets.ImageFolder(str(FRAMES_FLAT), transform=train_transform)\n",
    "num_samples = len(dataset)\n",
    "if num_samples == 0:\n",
    "    raise RuntimeError(f\"No images found in {FRAMES_FLAT}. Make sure Phase 2 created frames_flat/\")\n",
    "\n",
    "# create reproducible split\n",
    "val_ratio = 0.20\n",
    "num_val = int(num_samples * val_ratio)\n",
    "num_train = num_samples - num_val\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [num_train, num_val], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# override val dataset transform\n",
    "val_ds.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "num_classes = len(dataset.classes)\n",
    "print(f\"Found classes={num_classes}, total_images={num_samples}, train={num_train}, val={num_val}\")\n",
    "\n",
    "# ---- model ----\n",
    "model = models.resnet18(pretrained=USE_PRETRAINED)\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# ---- optimizer / scheduler / loss ----\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "# ---- training / eval functions ----\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    preds = []\n",
    "    trues = []\n",
    "    for inputs, labels in tqdm(loader, desc=\"Train Batches\", leave=False):\n",
    "        inputs = inputs.to(device); labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        preds.extend(torch.argmax(outputs.detach(), dim=1).cpu().tolist())\n",
    "        trues.extend(labels.cpu().tolist())\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    return sum(losses)/len(losses), acc\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader, desc=\"Val Batches\", leave=False):\n",
    "            inputs = inputs.to(device); labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "            preds.extend(torch.argmax(outputs, dim=1).cpu().tolist())\n",
    "            trues.extend(labels.cpu().tolist())\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    return sum(losses)/len(losses), acc\n",
    "\n",
    "# ---- main training loop ----\n",
    "best_val_acc = 0.0\n",
    "history = {\"epochs\": []}\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "    epoch_time = time.time() - t0\n",
    "\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS}  |  train_loss={train_loss:.4f} train_acc={train_acc:.4f}  |  val_loss={val_loss:.4f} val_acc={val_acc:.4f}  |  time={epoch_time:.1f}s\")\n",
    "    history[\"epochs\"].append({\n",
    "        \"epoch\": epoch, \"train_loss\": train_loss, \"train_acc\": train_acc,\n",
    "        \"val_loss\": val_loss, \"val_acc\": val_acc, \"time_s\": epoch_time\n",
    "    })\n",
    "\n",
    "    # checkpoint best\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            \"epoch\": epoch, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"val_acc\": val_acc, \"classes\": dataset.classes\n",
    "        }, CHECKPOINT_PATH)\n",
    "        print(f\"  -> Saved best checkpoint to {CHECKPOINT_PATH}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Training complete in {total_time/60:.2f} minutes. Best val_acc={best_val_acc:.4f}\")\n",
    "\n",
    "# write report\n",
    "report = {\"device\": str(device), \"num_classes\": num_classes, \"num_images\": num_samples, \"batch_size\": BATCH_SIZE,\n",
    "          \"epochs\": NUM_EPOCHS, \"best_val_acc\": best_val_acc, \"history\": history}\n",
    "with open(REPORT_PATH, \"w\") as fh:\n",
    "    json.dump(report, fh, indent=2)\n",
    "print(f\"Wrote training report -> {REPORT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c40892b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Creating dev subset at frames_dev\n",
      "Dev subset created with classes: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam']\n",
      "Dev images=1000, train=800, val=200, classes=5\n",
      "Epoch 1/3 | train_loss=0.8023 train_acc=0.7662 | val_loss=0.2164 val_acc=0.9950 | time=157.3s\n",
      " -> Saved dev_resnet18_best.pth\n",
      "Epoch 2/3 | train_loss=0.2405 train_acc=0.9700 | val_loss=0.0998 val_acc=0.9900 | time=105.4s\n",
      "Epoch 3/3 | train_loss=0.1358 train_acc=0.9812 | val_loss=0.0606 val_acc=1.0000 | time=82.8s\n",
      " -> Saved dev_resnet18_best.pth\n",
      "Dev training complete in 346.8s | best_val_acc=1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Quick dev-run: create small subset + freeze backbone + train few epochs (fast on CPU)\n",
    "# Paste & run in train.ipynb\n",
    "\n",
    "import shutil, random, time\n",
    "from pathlib import Path\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------- CONFIG (fast) --------\n",
    "SRC_FRAMES = Path(\"frames_flat\")\n",
    "DEV_ROOT   = Path(\"frames_dev\")      # new small dev folder (ImageFolder layout)\n",
    "NUM_CLASSES_DEV = 5                 # how many classes to sample for dev\n",
    "IMAGES_PER_CLASS = 200              # images per class\n",
    "IMG_SIZE = 112                      # smaller input -> faster\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 3\n",
    "LR = 1e-3\n",
    "NUM_WORKERS = 2\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# -------------------------------\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "# 1) Build dev subset\n",
    "if DEV_ROOT.exists():\n",
    "    print(\"frames_dev already exists — reusing it:\", DEV_ROOT.resolve())\n",
    "else:\n",
    "    print(\"Creating dev subset at\", DEV_ROOT)\n",
    "    DEV_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "    classes = sorted([p.name for p in SRC_FRAMES.iterdir() if p.is_dir()])[:NUM_CLASSES_DEV]\n",
    "    for cls in classes:\n",
    "        dst = DEV_ROOT / cls\n",
    "        dst.mkdir(parents=True, exist_ok=True)\n",
    "        src_cls = SRC_FRAMES / cls\n",
    "        imgs = sorted([p for p in src_cls.iterdir() if p.suffix.lower() == \".jpg\"])\n",
    "        if not imgs:\n",
    "            # if nested video folders exist (frames out originally nested), grab from frames/<class>/*/*.jpg\n",
    "            imgs = sorted([p for p in (Path(\"frames\")/cls).rglob(\"*.jpg\")])\n",
    "        sampled = imgs[:IMAGES_PER_CLASS]\n",
    "        for src in sampled:\n",
    "            dst_file = dst / src.name\n",
    "            if not dst_file.exists():\n",
    "                shutil.copy2(src, dst_file)\n",
    "    print(\"Dev subset created with classes:\", classes)\n",
    "\n",
    "# 2) Dataloaders\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(int(IMG_SIZE*1.15)),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(str(DEV_ROOT), transform=train_transform)\n",
    "n = len(dataset)\n",
    "if n == 0:\n",
    "    raise RuntimeError(\"No images in dev folder. Check paths.\")\n",
    "val_ratio = 0.2\n",
    "num_val = int(n*val_ratio)\n",
    "num_train = n - num_val\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [num_train, num_val], generator=torch.Generator().manual_seed(42))\n",
    "val_ds.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f\"Dev images={n}, train={num_train}, val={num_val}, classes={len(dataset.classes)}\")\n",
    "\n",
    "# 3) Model (resnet18) with frozen backbone\n",
    "model = models.resnet18(pretrained=True)\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, len(dataset.classes))   # only last layer trains\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 4) Train quick\n",
    "def run_epoch(train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "        loader = train_loader\n",
    "    else:\n",
    "        model.eval()\n",
    "        loader = val_loader\n",
    "    losses=[]\n",
    "    preds=[]; trues=[]\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for X,y in loader:\n",
    "            X = X.to(DEVICE); y = y.to(DEVICE)\n",
    "            out = model(X)\n",
    "            loss = criterion(out, y)\n",
    "            if train:\n",
    "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            preds += torch.argmax(out.detach(), dim=1).cpu().tolist()\n",
    "            trues += y.cpu().tolist()\n",
    "    acc = accuracy_score(trues, preds) if trues else 0.0\n",
    "    return sum(losses)/len(losses), acc\n",
    "\n",
    "best_val = 0.0\n",
    "start = time.time()\n",
    "for ep in range(1, NUM_EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    tr_loss, tr_acc = run_epoch(train=True)\n",
    "    val_loss, val_acc = run_epoch(train=False)\n",
    "    took = time.time() - t0\n",
    "    print(f\"Epoch {ep}/{NUM_EPOCHS} | train_loss={tr_loss:.4f} train_acc={tr_acc:.4f} | val_loss={val_loss:.4f} val_acc={val_acc:.4f} | time={took:.1f}s\")\n",
    "    if val_acc > best_val:\n",
    "        best_val = val_acc\n",
    "        torch.save({\"model_state\": model.state_dict(), \"classes\": dataset.classes}, \"dev_resnet18_best.pth\")\n",
    "        print(\" -> Saved dev_resnet18_best.pth\")\n",
    "total = time.time()-start\n",
    "print(f\"Dev training complete in {total:.1f}s | best_val_acc={best_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8831a01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu | batch_size: 8\n",
      "Images=35375 | Classes=101 | Train=28300 | Val=7075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7208\\3152293568.py:97: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type==\"cuda\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: training head only (fc) for 3 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage1 ep1:   0%|          | 0/3538 [00:00<?, ?it/s]C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7208\\3152293568.py:110: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\")):\n",
      "                                                                 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 137\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model.fc.parameters(): p.requires_grad = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStage 1: training head only (fc) for\u001b[39m\u001b[33m\"\u001b[39m, N_FREEZE, \u001b[33m\"\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m hist1 = \u001b[43mtrain_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_FREEZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR_HEAD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mStage1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# save intermediate\u001b[39;00m\n\u001b[32m    140\u001b[39m torch.save({\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mhead_trained\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmodel_state\u001b[39m\u001b[33m\"\u001b[39m:model.state_dict(),\u001b[33m\"\u001b[39m\u001b[33mclasses\u001b[39m\u001b[33m\"\u001b[39m:dataset.classes}, CHECKPOINT_INT)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 121\u001b[39m, in \u001b[36mtrain_stage\u001b[39m\u001b[34m(model, params, epochs, lr, desc)\u001b[39m\n\u001b[32m    119\u001b[39m tr_loss = \u001b[38;5;28msum\u001b[39m(losses)/\u001b[38;5;28mlen\u001b[39m(losses) \u001b[38;5;28;01mif\u001b[39;00m losses \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m    120\u001b[39m tr_acc  = accuracy_score(trues, preds) \u001b[38;5;28;01mif\u001b[39;00m trues \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m val_loss, val_acc = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m took = time.time()-t0\n\u001b[32m    123\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdesc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | tr_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tr_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | val_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m val_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | time=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtook\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(model, loader, device)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m X,y \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[32m     88\u001b[39m     X,y = X.to(device), y.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m     loss = crit(out,y)\n\u001b[32m     91\u001b[39m     losses.append(loss.item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[39m, in \u001b[36mResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\resnet.py:275\u001b[39m, in \u001b[36mResNet._forward_impl\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    273\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer1(x)\n\u001b[32m    274\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer2(x)\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer4(x)\n\u001b[32m    278\u001b[39m x = \u001b[38;5;28mself\u001b[39m.avgpool(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\resnet.py:96\u001b[39m, in \u001b[36mBasicBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     93\u001b[39m out = \u001b[38;5;28mself\u001b[39m.bn1(out)\n\u001b[32m     94\u001b[39m out = \u001b[38;5;28mself\u001b[39m.relu(out)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m out = \u001b[38;5;28mself\u001b[39m.bn2(out)\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.downsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Phase 3B — Progressive fine-tune on FULL frames_flat (paper-quality baseline, CPU-safe, GPU-ready)\n",
    "# What this does (quick):\n",
    "# 1) Train only the new classification head for N_FREEZE epochs.\n",
    "# 2) Unfreeze layer4+fc and fine-tune for N_UNFREEZE epochs.\n",
    "# Saves: resnet_progressive_best.pth and phase3_progressive_report.json\n",
    "\n",
    "import os, json, time, math, random\n",
    "from pathlib import Path\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============= CONFIG =============\n",
    "FRAMES_FLAT = Path(\"frames_flat\")\n",
    "IMG_SIZE = 224\n",
    "SEED = 42\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 8 if DEVICE.type == \"cpu\" else 32       # CPU-safe\n",
    "NUM_WORKERS = 2 if DEVICE.type == \"cpu\" else 4\n",
    "\n",
    "N_FREEZE = 3                 # head-only epochs\n",
    "N_UNFREEZE = 7               # layer4+fc epochs\n",
    "LR_HEAD = 1e-3\n",
    "LR_FINETUNE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "ACCUM_STEPS = 1 if DEVICE.type == \"cpu\" else 1       # set >1 if you need gradient accumulation\n",
    "\n",
    "CHECKPOINT_BEST = \"resnet_progressive_best.pth\"\n",
    "CHECKPOINT_INT  = \"resnet_head_trained.pth\"\n",
    "REPORT_PATH = \"phase3_progressive_report.json\"\n",
    "# ==================================\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = (DEVICE.type == \"cuda\")\n",
    "set_seed(SEED)\n",
    "\n",
    "print(\"Device:\", DEVICE, \"| batch_size:\", BATCH_SIZE)\n",
    "\n",
    "# ----- Datasets & Loaders -----\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize(int(IMG_SIZE*1.15)),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(str(FRAMES_FLAT), transform=train_tf)\n",
    "if len(dataset) == 0:\n",
    "    raise RuntimeError(f\"No images found in {FRAMES_FLAT}. Run Phase 2 first.\")\n",
    "num_val = int(0.20 * len(dataset))\n",
    "num_train = len(dataset) - num_val\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [num_train, num_val], generator=torch.Generator().manual_seed(SEED))\n",
    "val_ds.dataset.transform = val_tf\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=(DEVICE.type==\"cuda\"))\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=(DEVICE.type==\"cuda\"))\n",
    "\n",
    "num_classes = len(dataset.classes)\n",
    "print(f\"Images={len(dataset)} | Classes={num_classes} | Train={num_train} | Val={num_val}\")\n",
    "\n",
    "# ----- Model -----\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "model = resnet18(weights=weights)\n",
    "in_f = model.fc.in_features\n",
    "model.fc = nn.Linear(in_f, num_classes)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Utility: eval loop\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    losses=[]; preds=[]; trues=[]\n",
    "    with torch.no_grad():\n",
    "        for X,y in loader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            out = model(X)\n",
    "            loss = crit(out,y)\n",
    "            losses.append(loss.item())\n",
    "            preds += torch.argmax(out, dim=1).cpu().tolist()\n",
    "            trues += y.cpu().tolist()\n",
    "    return (sum(losses)/len(losses) if losses else 0.0), (accuracy_score(trues,preds) if trues else 0.0)\n",
    "\n",
    "# Training helper with (optional) grad accumulation and AMP on GPU\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type==\"cuda\"))\n",
    "def train_stage(model, params, epochs, lr, desc):\n",
    "    optimizer = optim.AdamW(params, lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    history=[]\n",
    "    global_best = {\"val_acc\": -1.0}\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        losses=[]; preds=[]; trues=[]\n",
    "        t0 = time.time()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        for i,(X,y) in enumerate(tqdm(train_loader, desc=f\"{desc} ep{ep}\", leave=False)):\n",
    "            X,y = X.to(DEVICE), y.to(DEVICE)\n",
    "            with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\")):\n",
    "                out = model(X)\n",
    "                loss = crit(out,y) / ACCUM_STEPS\n",
    "            scaler.scale(loss).backward()\n",
    "            if (i+1) % ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer); scaler.update(); optimizer.zero_grad(set_to_none=True)\n",
    "            losses.append(loss.item()*ACCUM_STEPS)\n",
    "            preds += torch.argmax(out.detach(), dim=1).cpu().tolist()\n",
    "            trues += y.cpu().tolist()\n",
    "        tr_loss = sum(losses)/len(losses) if losses else 0.0\n",
    "        tr_acc  = accuracy_score(trues, preds) if trues else 0.0\n",
    "        val_loss, val_acc = evaluate(model, val_loader, DEVICE)\n",
    "        took = time.time()-t0\n",
    "        print(f\"{desc} {ep}/{epochs} | tr_loss={tr_loss:.4f} tr_acc={tr_acc:.4f} | val_loss={val_loss:.4f} val_acc={val_acc:.4f} | time={took:.1f}s\")\n",
    "        history.append({\"epoch\": ep, \"tr_loss\": tr_loss, \"tr_acc\": tr_acc, \"val_loss\": val_loss, \"val_acc\": val_acc, \"time_s\": took})\n",
    "\n",
    "        # checkpoint best\n",
    "        if val_acc > global_best.get(\"val_acc\", -1):\n",
    "            global_best.update({\"val_acc\": val_acc, \"epoch\": ep})\n",
    "            torch.save({\"stage\": desc, \"epoch\": ep, \"model_state\": model.state_dict(), \"classes\": dataset.classes}, CHECKPOINT_BEST)\n",
    "            print(f\" -> Saved best checkpoint: {CHECKPOINT_BEST} (val_acc={val_acc:.4f})\")\n",
    "    return history\n",
    "\n",
    "# -------- Stage 1: freeze backbone, train FC --------\n",
    "for p in model.parameters(): p.requires_grad = False\n",
    "for p in model.fc.parameters(): p.requires_grad = True\n",
    "print(\"Stage 1: training head only (fc) for\", N_FREEZE, \"epochs\")\n",
    "hist1 = train_stage(model, model.fc.parameters(), N_FREEZE, LR_HEAD, \"Stage1\")\n",
    "\n",
    "# save intermediate\n",
    "torch.save({\"stage\":\"head_trained\",\"model_state\":model.state_dict(),\"classes\":dataset.classes}, CHECKPOINT_INT)\n",
    "print(\"Saved intermediate:\", CHECKPOINT_INT)\n",
    "\n",
    "# -------- Stage 2: unfreeze layer4 + fc, fine-tune --------\n",
    "for name, p in model.named_parameters():\n",
    "    if name.startswith(\"layer4.\") or name.startswith(\"fc.\"):\n",
    "        p.requires_grad = True\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "\n",
    "params_to_update = [p for p in model.parameters() if p.requires_grad]\n",
    "print(\"Stage 2: fine-tuning layer4+fc for\", N_UNFREEZE, \"epochs\")\n",
    "hist2 = train_stage(model, params_to_update, N_UNFREEZE, LR_FINETUNE, \"Stage2\")\n",
    "\n",
    "# Final eval + save\n",
    "final_val_loss, final_val_acc = evaluate(model, val_loader, DEVICE)\n",
    "torch.save({\"stage\":\"progressive_final\",\"model_state\":model.state_dict(),\"val_acc\":final_val_acc,\"classes\":dataset.classes}, CHECKPOINT_BEST)\n",
    "print(f\"Saved FINAL checkpoint -> {CHECKPOINT_BEST} | final_val_acc={final_val_acc:.4f}\")\n",
    "\n",
    "# Report\n",
    "report = {\n",
    "    \"device\": str(DEVICE), \"batch_size\": BATCH_SIZE, \"num_workers\": NUM_WORKERS,\n",
    "    \"img_size\": IMG_SIZE, \"num_images\": len(dataset), \"num_classes\": num_classes,\n",
    "    \"stage1_epochs\": N_FREEZE, \"stage2_epochs\": N_UNFREEZE,\n",
    "    \"lr_head\": LR_HEAD, \"lr_finetune\": LR_FINETUNE,\n",
    "    \"weight_decay\": WEIGHT_DECAY, \"accum_steps\": ACCUM_STEPS,\n",
    "    \"history_stage1\": hist1, \"history_stage2\": hist2,\n",
    "    \"final_val_acc\": final_val_acc\n",
    "}\n",
    "with open(REPORT_PATH, \"w\") as f: json.dump(report, f, indent=2)\n",
    "print(\"Wrote report:\", REPORT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5121c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pennylane pennylane-lightning torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4657fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4 — QCNN starter (hybrid ResNet18 feature extractor + PennyLane quantum layer)\n",
    "# Paste & run in train.ipynb. Defaults use frames_dev (small). Adjust CONFIG to use frames_flat if you understand cost.\n",
    "\n",
    "import time, json, random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PennyLane imports\n",
    "import pennylane as qml\n",
    "from pennylane import qnode\n",
    "from pennylane import numpy as pnp\n",
    "from pennylane.qnn import TorchLayer\n",
    "\n",
    "# ----------------------- CONFIG (change as needed) -----------------------\n",
    "DATA_ROOT = Path(\"frames_dev\")      # USE frames_dev for QCNN experiments (fast). Use frames_flat only if you know the cost.\n",
    "IMG_SIZE = 224\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 8 if DEVICE.type == \"cpu\" else 32\n",
    "NUM_WORKERS = 2\n",
    "SEED = 42\n",
    "\n",
    "# Quantum circuit params\n",
    "N_QUBITS = 6           # start small (4-8 qubits). More qubits => heavier simulation cost\n",
    "N_Q_LAYERS = 3         # variational layers in the circuit\n",
    "Q_OUTPUTS = N_QUBITS   # we will read one expectation per qubit\n",
    "\n",
    "EPOCHS = 8\n",
    "LR = 2e-4\n",
    "\n",
    "CHECKPOINT = \"qcnn_hybrid_best.pth\"\n",
    "REPORT = \"phase4_qcnn_report.json\"\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(\"Device:\", DEVICE, \"| data root:\", DATA_ROOT, \"| batch_size:\", BATCH_SIZE)\n",
    "if not DATA_ROOT.exists():\n",
    "    raise RuntimeError(f\"Data folder not found: {DATA_ROOT} (create using Phase2/frames_dev)\")\n",
    "\n",
    "# ------------------ Data / Dataloaders ------------------\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize(int(IMG_SIZE*1.15)),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(str(DATA_ROOT), transform=train_tf)\n",
    "if len(dataset) == 0:\n",
    "    raise RuntimeError(f\"No images in {DATA_ROOT}. Run Phase2 to create frames_dev or frames_flat.\")\n",
    "\n",
    "num_val = int(0.20 * len(dataset))\n",
    "num_train = len(dataset) - num_val\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [num_train, num_val], generator=torch.Generator().manual_seed(SEED))\n",
    "val_ds.dataset.transform = val_tf\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=(DEVICE.type==\"cuda\"))\n",
    "val_loader = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=(DEVICE.type==\"cuda\"))\n",
    "\n",
    "num_classes = len(dataset.classes)\n",
    "print(f\"Images={len(dataset)} | Classes={num_classes} | Train={num_train} | Val={num_val}\")\n",
    "\n",
    "# ------------------ Feature extractor (frozen) ------------------\n",
    "# Use ResNet18 up to the avgpool to get a 512-d feature vector\n",
    "resnet_weights = ResNet18_Weights.DEFAULT\n",
    "fe = resnet18(weights=resnet_weights)\n",
    "# remove final fc: keep everything up to avgpool\n",
    "modules = list(fe.children())[:-1]  # all layers except final fc\n",
    "feature_extractor = nn.Sequential(*modules)  # outputs shape [B, 512, 1, 1]\n",
    "for p in feature_extractor.parameters():\n",
    "    p.requires_grad = False\n",
    "feature_extractor = feature_extractor.to(DEVICE)\n",
    "\n",
    "# a small wrapper to flatten feature maps to vector\n",
    "class FeatureFlatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        # x: [B, 512, 1, 1] typical after avgpool; flatten to [B, 512]\n",
    "        return torch.flatten(x, 1)\n",
    "\n",
    "flatten = FeatureFlatten()\n",
    "\n",
    "# ------------------ Classical reducer -> quantum input ------------------\n",
    "# Reduce 512 -> N_QUBITS (scale down features before quantum encoding)\n",
    "class Reducer(nn.Module):\n",
    "    def __init__(self, in_features=512, out_features=N_QUBITS):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.act = nn.Tanh()  # keep values in [-1,1] useful for angle encoding\n",
    "    def forward(self, x):\n",
    "        # x assumed shape [B, 512] (float32)\n",
    "        return self.act(self.fc(x))\n",
    "\n",
    "reducer = Reducer().to(DEVICE)\n",
    "\n",
    "# ------------------ Quantum circuit (PennyLane) ------------------\n",
    "dev = qml.device(\"default.qubit\", wires=N_QUBITS, shots=None)  # statevector simulator\n",
    "\n",
    "# Define the variational circuit: angle embedding + StronglyEntanglingLayers\n",
    "def qnode_circuit(inputs, weights):\n",
    "    # inputs: length = N_QUBITS (torch tensor)\n",
    "    # weights: shape (N_Q_LAYERS, N_QUBITS, 3) for StronglyEntanglingLayers\n",
    "    qml.AngleEmbedding(inputs, wires=range(N_QUBITS), rotation='Y')  # embed via Y-rotations\n",
    "    qml.templates.StronglyEntanglingLayers(weights, wires=range(N_QUBITS))\n",
    "    # return expectation values for each qubit (Z)\n",
    "    return [qml.expval(qml.PauliZ(w)) for w in range(N_QUBITS)]\n",
    "\n",
    "# weight_shapes mapping required by TorchLayer\n",
    "weight_shapes = {\"weights\": (N_Q_LAYERS, N_QUBITS, 3)}\n",
    "\n",
    "# Create QNode (interface='torch' so it consumes/returns torch tensors)\n",
    "qnode_torch = qml.QNode(qnode_circuit, dev, interface=\"torch\", diff_method=\"backprop\")\n",
    "\n",
    "# Wrap as a TorchLayer\n",
    "quantum_layer = TorchLayer(qnode_torch, weight_shapes).to(DEVICE)  # outputs tensor shape [B, N_QUBITS]\n",
    "\n",
    "# ------------------ Hybrid model assembly ------------------\n",
    "class HybridQCNN(nn.Module):\n",
    "    def __init__(self, feature_extractor, flatten, reducer, quantum_layer, n_qubits=N_QUBITS, num_classes=num_classes):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.flatten = flatten\n",
    "        self.reducer = reducer\n",
    "        self.quantum_layer = quantum_layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(n_qubits, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, H, W]\n",
    "        feat = self.feature_extractor(x)           # [B, 512, 1, 1]\n",
    "        feat = self.flatten(feat)                  # [B, 512]\n",
    "        q_in = self.reducer(feat)                  # [B, N_QUBITS] values in (-1,1)\n",
    "        # quantum layer expects shape [B, N_QUBITS] -> returns [B, N_QUBITS] (expectations)\n",
    "        q_out = self.quantum_layer(q_in)\n",
    "        out = self.classifier(q_out)\n",
    "        return out\n",
    "\n",
    "model = HybridQCNN(feature_extractor, flatten, reducer, quantum_layer, n_qubits=N_QUBITS, num_classes=num_classes).to(DEVICE)\n",
    "\n",
    "# ------------------ Training setup ------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    losses = []; preds = []; trues = []\n",
    "    with torch.no_grad():\n",
    "        for X,y in loader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            out = model(X)\n",
    "            loss = criterion(out, y)\n",
    "            losses.append(loss.item())\n",
    "            preds += torch.argmax(out, dim=1).cpu().tolist()\n",
    "            trues += y.cpu().tolist()\n",
    "    return (sum(losses)/len(losses) if losses else 0.0), (accuracy_score(trues, preds) if trues else 0.0)\n",
    "\n",
    "# ------------------ Training loop ------------------\n",
    "best_val_acc = 0.0\n",
    "history = {\"epochs\": []}\n",
    "start_all = time.time()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    running_losses = []\n",
    "    train_preds = []; train_trues = []\n",
    "    for X,y in tqdm(train_loader, desc=f\"Train epoch {epoch}\", leave=False):\n",
    "        X,y = X.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_losses.append(loss.item())\n",
    "        train_preds += torch.argmax(out.detach(), dim=1).cpu().tolist()\n",
    "        train_trues += y.cpu().tolist()\n",
    "\n",
    "    train_loss = sum(running_losses)/len(running_losses) if running_losses else 0.0\n",
    "    train_acc = accuracy_score(train_trues, train_preds) if train_trues else 0.0\n",
    "    val_loss, val_acc = evaluate(model, val_loader, DEVICE)\n",
    "    epoch_time = time.time()-t0\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | tr_loss={train_loss:.4f} tr_acc={train_acc:.4f} | val_loss={val_loss:.4f} val_acc={val_acc:.4f} | time={epoch_time:.1f}s\")\n",
    "    history[\"epochs\"].append({\"epoch\": epoch, \"tr_loss\": train_loss, \"tr_acc\": train_acc, \"val_loss\": val_loss, \"val_acc\": val_acc, \"time_s\": epoch_time})\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\"epoch\": epoch, \"model_state\": model.state_dict(), \"val_acc\": val_acc, \"classes\": dataset.classes}, CHECKPOINT)\n",
    "        print(f\" -> Saved checkpoint: {CHECKPOINT}\")\n",
    "\n",
    "total_time = time.time() - start_all\n",
    "print(f\"QCNN training finished in {total_time/60:.2f} minutes. Best val_acc={best_val_acc:.4f}\")\n",
    "\n",
    "# ------------------ Save report ------------------\n",
    "report = {\n",
    "    \"device\": str(DEVICE), \"n_qubits\": N_QUBITS, \"n_q_layers\": N_Q_LAYERS, \"epochs\": EPOCHS,\n",
    "    \"best_val_acc\": best_val_acc, \"history\": history, \"num_images\": len(dataset), \"num_classes\": num_classes\n",
    "}\n",
    "with open(REPORT, \"w\") as fh:\n",
    "    json.dump(report, fh, indent=2)\n",
    "print(\"Wrote report:\", REPORT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f84ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5 — QGCNN (Quantum-Gated CNN): hybrid ResNet feature extractor + quantum gate layer that modulates features\n",
    "# Paste & run in train.ipynb. Recommended to run on frames_dev (small) first.\n",
    "\n",
    "import time, json, random\n",
    "from pathlib import Path\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pennylane imports\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "from pennylane.qnn import TorchLayer\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "DATA_ROOT = Path(\"frames_dev\")       # start with small dev; change to frames_flat later\n",
    "IMG_SIZE = 224\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 8 if DEVICE.type == \"cpu\" else 32\n",
    "NUM_WORKERS = 2\n",
    "SEED = 42\n",
    "\n",
    "# Quantum gate settings\n",
    "N_QUBITS = 4         # fewer qubits => much faster; increase to experiment\n",
    "N_Q_LAYERS = 2\n",
    "ENCODING = \"AngleEmbedding\"   # embedding style\n",
    "EPOCHS = 8\n",
    "LR = 2e-4\n",
    "\n",
    "CHECKPOINT = \"qgcnn_hybrid_best.pth\"\n",
    "REPORT = \"phase5_qgcnn_report.json\"\n",
    "# ----------------------------------------\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(\"Device:\", DEVICE, \" | Data root:\", DATA_ROOT, \" | batch_size:\", BATCH_SIZE)\n",
    "\n",
    "# ---- Data ----\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize(int(IMG_SIZE*1.15)),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(str(DATA_ROOT), transform=train_tf)\n",
    "if len(dataset) == 0:\n",
    "    raise RuntimeError(f\"No images found in {DATA_ROOT}. Create frames_dev or use frames_flat from Phase 2.\")\n",
    "\n",
    "num_val = int(0.2 * len(dataset))\n",
    "num_train = len(dataset) - num_val\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [num_train, num_val], generator=torch.Generator().manual_seed(SEED))\n",
    "val_ds.dataset.transform = val_tf\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=(DEVICE.type==\"cuda\"))\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=(DEVICE.type==\"cuda\"))\n",
    "\n",
    "num_classes = len(dataset.classes)\n",
    "print(f\"Images={len(dataset)} | Classes={num_classes} | Train={num_train} | Val={num_val}\")\n",
    "\n",
    "# ---- Feature extractor (frozen ResNet up to avgpool) ----\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "backbone = resnet18(weights=weights)\n",
    "modules = list(backbone.children())[:-1]\n",
    "feature_extractor = nn.Sequential(*modules)   # outputs [B,512,1,1]\n",
    "for p in feature_extractor.parameters():\n",
    "    p.requires_grad = False\n",
    "feature_extractor = feature_extractor.to(DEVICE)\n",
    "\n",
    "# flatten helper\n",
    "class FlattenFeat(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.flatten(x, 1)\n",
    "flatten = FlattenFeat()\n",
    "\n",
    "# ---- Classical reducer: 512 -> gate_dim (equal to N_QUBITS) ----\n",
    "class Reducer(nn.Module):\n",
    "    def __init__(self, in_dim=512, out_dim=N_QUBITS):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, out_dim)\n",
    "        self.act = nn.Tanh()\n",
    "    def forward(self, x):\n",
    "        return self.act(self.fc(x))\n",
    "\n",
    "reducer = Reducer().to(DEVICE)\n",
    "\n",
    "# ---- Quantum gate circuit ----\n",
    "dev = qml.device(\"default.qubit\", wires=N_QUBITS, shots=None)\n",
    "\n",
    "# circuit: embed -> variational layers -> return Z expectations\n",
    "def qnode_circuit(inputs, weights):\n",
    "    # inputs length = N_QUBITS\n",
    "    qml.AngleEmbedding(inputs, wires=range(N_QUBITS), rotation='Y')\n",
    "    qml.templates.StronglyEntanglingLayers(weights, wires=range(N_QUBITS))\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(N_QUBITS)]\n",
    "\n",
    "weight_shapes = {\"weights\": (N_Q_LAYERS, N_QUBITS, 3)}\n",
    "qnode = qml.QNode(qnode_circuit, dev, interface=\"torch\", diff_method=\"backprop\")\n",
    "quantum_layer = TorchLayer(qnode, weight_shapes).to(DEVICE)   # outputs shape [B, N_QUBITS] with values in [-1,1]\n",
    "\n",
    "# ---- Quantum-Gated Block: apply quantum gates (sigmoid -> [0,1]) to modulate classical features ----\n",
    "class QuantumGatedBlock(nn.Module):\n",
    "    def __init__(self, reducer, quantum_layer, in_dim=512, out_dim=128, n_qubits=N_QUBITS):\n",
    "        super().__init__()\n",
    "        self.reducer = reducer            # maps 512 -> n_qubits (tanh)\n",
    "        self.quantum_layer = quantum_layer\n",
    "        # also create a small projection of the original features down to n_qubits to multiply with gates\n",
    "        self.proj = nn.Linear(in_dim, n_qubits)\n",
    "        self.post = nn.Sequential(\n",
    "            nn.Linear(n_qubits, out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "    def forward(self, feat):  # feat: [B,512]\n",
    "        red = self.reducer(feat)             # [B, n_qubits], in (-1,1)\n",
    "        q_out = self.quantum_layer(red)      # [B, n_qubits] in approximately (-1,1) (expectations)\n",
    "        gates = torch.sigmoid(q_out)         # convert to [0,1] gating values\n",
    "        proj_feat = self.proj(feat)          # [B, n_qubits]\n",
    "        gated = proj_feat * gates            # elementwise modulation\n",
    "        out = self.post(gated)               # [B, out_dim]\n",
    "        return out, gates                     # return gates optionally for inspection\n",
    "\n",
    "qg_block = QuantumGatedBlock(reducer, quantum_layer, in_dim=512, out_dim=128, n_qubits=N_QUBITS).to(DEVICE)\n",
    "\n",
    "# ---- Full QGCNN model ----\n",
    "class QGCNNHybrid(nn.Module):\n",
    "    def __init__(self, feat_ext, flatten, qg_block, num_classes):\n",
    "        super().__init__()\n",
    "        self.feat_ext = feat_ext\n",
    "        self.flatten = flatten\n",
    "        self.qg_block = qg_block\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        f = self.feat_ext(x)        # [B,512,1,1]\n",
    "        f = self.flatten(f)         # [B,512]\n",
    "        out_qg, gates = self.qg_block(f)   # out_qg [B,128]\n",
    "        logits = self.classifier(out_qg)\n",
    "        return logits, gates\n",
    "\n",
    "model = QGCNNHybrid(feature_extractor, flatten, qg_block, num_classes=num_classes).to(DEVICE)\n",
    "\n",
    "# ---- Training setup ----\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    losses=[]; preds=[]; trues=[]\n",
    "    with torch.no_grad():\n",
    "        for X,y in loader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            out, _ = model(X)\n",
    "            loss = criterion(out, y)\n",
    "            losses.append(loss.item())\n",
    "            preds += torch.argmax(out, dim=1).cpu().tolist()\n",
    "            trues += y.cpu().tolist()\n",
    "    return (sum(losses)/len(losses) if losses else 0.0), (accuracy_score(trues, preds) if trues else 0.0)\n",
    "\n",
    "# ---- Training loop ----\n",
    "best_val_acc = 0.0\n",
    "history = {\"epochs\": []}\n",
    "start_all = time.time()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    running_losses=[]; tr_preds=[]; tr_trues=[]\n",
    "    for X,y in tqdm(train_loader, desc=f\"Train epoch {epoch}\", leave=False):\n",
    "        X,y = X.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out, gates = model(X)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_losses.append(loss.item())\n",
    "        tr_preds += torch.argmax(out.detach(), dim=1).cpu().tolist()\n",
    "        tr_trues += y.cpu().tolist()\n",
    "    train_loss = sum(running_losses)/len(running_losses) if running_losses else 0.0\n",
    "    train_acc  = accuracy_score(tr_trues, tr_preds) if tr_trues else 0.0\n",
    "    val_loss, val_acc = evaluate_model(model, val_loader, DEVICE)\n",
    "    epoch_time = time.time()-t0\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | tr_loss={train_loss:.4f} tr_acc={train_acc:.4f} | val_loss={val_loss:.4f} val_acc={val_acc:.4f} | time={epoch_time:.1f}s\")\n",
    "    history[\"epochs\"].append({\"epoch\":epoch,\"tr_loss\":train_loss,\"tr_acc\":train_acc,\"val_loss\":val_loss,\"val_acc\":val_acc,\"time_s\":epoch_time})\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\"epoch\": epoch, \"model_state\": model.state_dict(), \"val_acc\": val_acc, \"classes\": dataset.classes}, CHECKPOINT)\n",
    "        print(f\" -> Saved checkpoint: {CHECKPOINT}\")\n",
    "\n",
    "total_time = time.time()-start_all\n",
    "print(f\"QGCNN training finished in {total_time/60:.2f} minutes. Best val_acc={best_val_acc:.4f}\")\n",
    "\n",
    "# ---- Save report ----\n",
    "report = {\n",
    "    \"device\": str(DEVICE), \"n_qubits\": N_QUBITS, \"n_q_layers\": N_Q_LAYERS, \"epochs\": EPOCHS,\n",
    "    \"best_val_acc\": best_val_acc, \"history\": history, \"num_images\": len(dataset), \"num_classes\": num_classes\n",
    "}\n",
    "with open(REPORT, \"w\") as fh:\n",
    "    json.dump(report, fh, indent=2)\n",
    "print(\"Wrote report:\", REPORT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cdff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 6 — Evaluation & Comparison script\n",
    "# Paste & run in train.ipynb (after training baseline, QCNN, QGCNN).\n",
    "# Outputs: comparison_report.csv, comparison_plots.png, confusion_{model}.png, and phase6_report.json\n",
    "\n",
    "import time, json, os\n",
    "from pathlib import Path\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "FRAMES_FLAT = Path(\"frames_flat\")   # dataset used for final evaluation (same used in training splits)\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# checkpoint paths (change if you used different names)\n",
    "CKPT_CNN  = Path(\"resnet_progressive_best.pth\")\n",
    "CKPT_QCNN = Path(\"qcnn_hybrid_best.pth\")\n",
    "CKPT_QGCNN= Path(\"qgcnn_hybrid_best.pth\")\n",
    "\n",
    "REPORT_CSV = \"comparison_report.csv\"\n",
    "PLOT_PNG = \"comparison_plots.png\"\n",
    "OUT_JSON = \"phase6_report.json\"\n",
    "# ---------------------------------------\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ---- Prepare dataset (use same val split method as training) ----\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize(int(IMG_SIZE*1.15)),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(str(FRAMES_FLAT), transform=transform_val)\n",
    "if len(dataset) == 0:\n",
    "    raise RuntimeError(\"No images found in frames_flat. Use Phase2 output.\")\n",
    "\n",
    "# create deterministic train/val split identical to training (seed 42)\n",
    "val_ratio = 0.20\n",
    "num_val = int(len(dataset)*val_ratio)\n",
    "num_train = len(dataset) - num_val\n",
    "_, val_ds = torch.utils.data.random_split(dataset, [num_train, num_val], generator=torch.Generator().manual_seed(42))\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=(DEVICE.type==\"cuda\"))\n",
    "\n",
    "classes = dataset.classes\n",
    "n_classes = len(classes)\n",
    "print(f\"Evaluation set size: {len(val_ds)} images | classes: {n_classes}\")\n",
    "\n",
    "# ---- Helpers ----\n",
    "def param_count(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def load_cnn_model(checkpoint_path, device):\n",
    "    # ResNet18 baseline architecture (same as used in training)\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    in_f = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_f, n_classes)\n",
    "    model = model.to(device)\n",
    "    if checkpoint_path.exists():\n",
    "        ck = torch.load(checkpoint_path, map_location=device)\n",
    "        # checkpoint format: (we saved dict with model_state)\n",
    "        if \"model_state\" in ck:\n",
    "            model.load_state_dict(ck[\"model_state\"])\n",
    "        elif \"model_state_dict\" in ck:\n",
    "            model.load_state_dict(ck[\"model_state_dict\"])\n",
    "        elif \"model_state_dict\" not in ck and \"model_state\" not in ck and \"epoch\" in ck and \"model_state_dict\" in ck:\n",
    "            model.load_state_dict(ck['model_state_dict'])\n",
    "        else:\n",
    "            # attempt to load all keys (fallback)\n",
    "            try:\n",
    "                model.load_state_dict(ck)\n",
    "            except Exception:\n",
    "                print(\"Warning: unable to load full checkpoint for CNN; proceeding with random init.\")\n",
    "    else:\n",
    "        print(\"Warning: CNN checkpoint not found:\", checkpoint_path)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# QCNN and QGCNN model constructors must match your Phase4/Phase5 classes.\n",
    "# We'll re-declare minimal wrappers matching their definitions used earlier so we can load saved states.\n",
    "\n",
    "# ------- QCNN wrapper used in Phase 4 (must match) -------\n",
    "import pennylane as qml\n",
    "from pennylane.qnn import TorchLayer\n",
    "def build_qcnn_model(n_qubits=6, n_q_layers=3):\n",
    "    # feature extractor\n",
    "    fe = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    modules = list(fe.children())[:-1]\n",
    "    feature_extractor = nn.Sequential(*modules)\n",
    "    for p in feature_extractor.parameters(): p.requires_grad = False\n",
    "    flatten = nn.Flatten()\n",
    "    # reducer\n",
    "    class Reducer(nn.Module):\n",
    "        def __init__(self, in_features=512, out_features=n_qubits):\n",
    "            super().__init__()\n",
    "            self.fc = nn.Linear(in_features, out_features)\n",
    "            self.act = nn.Tanh()\n",
    "        def forward(self,x):\n",
    "            return self.act(self.fc(x))\n",
    "    reducer = Reducer()\n",
    "    # quantum layer (rebuild like Phase4)\n",
    "    dev = qml.device(\"default.qubit\", wires=n_qubits, shots=None)\n",
    "    def qnode_circuit(inputs, weights):\n",
    "        qml.AngleEmbedding(inputs, wires=range(n_qubits), rotation='Y')\n",
    "        qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
    "    weight_shapes = {\"weights\": (n_q_layers, n_qubits, 3)}\n",
    "    qnode_torch = qml.QNode(qnode_circuit, dev, interface=\"torch\", diff_method=\"backprop\")\n",
    "    qlayer = TorchLayer(qnode_torch, weight_shapes)\n",
    "    # classifier\n",
    "    class HybridQCNN(nn.Module):\n",
    "        def __init__(self, feat_ext, flatten, reducer, qlayer, n_qubits, n_classes):\n",
    "            super().__init__()\n",
    "            self.feat_ext = feat_ext\n",
    "            self.flatten = flatten\n",
    "            self.reducer = reducer\n",
    "            self.q = qlayer\n",
    "            self.classifier = nn.Sequential(nn.Linear(n_qubits,64), nn.ReLU(), nn.Dropout(0.3), nn.Linear(64, n_classes))\n",
    "        def forward(self, x):\n",
    "            feat = self.feat_ext(x)\n",
    "            feat = self.flatten(feat)\n",
    "            qin = self.reducer(feat)\n",
    "            qout = self.q(qin)\n",
    "            out = self.classifier(qout)\n",
    "            return out\n",
    "    return HybridQCNN(feature_extractor, flatten, reducer, qlayer, n_qubits, n_classes)\n",
    "\n",
    "# ------- QGCNN wrapper used in Phase 5 (must match) -------\n",
    "def build_qgcnn_model(n_qubits=4, n_q_layers=2):\n",
    "    fe = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    modules = list(fe.children())[:-1]\n",
    "    feature_extractor = nn.Sequential(*modules)\n",
    "    for p in feature_extractor.parameters(): p.requires_grad = False\n",
    "    flatten = nn.Flatten()\n",
    "    class Reducer(nn.Module):\n",
    "        def __init__(self, in_dim=512, out_dim=n_qubits):\n",
    "            super().__init__()\n",
    "            self.fc = nn.Linear(in_dim, out_dim); self.act = nn.Tanh()\n",
    "        def forward(self, x): return self.act(self.fc(x))\n",
    "    reducer = Reducer()\n",
    "    dev = qml.device(\"default.qubit\", wires=n_qubits, shots=None)\n",
    "    def qnode_circuit(inputs, weights):\n",
    "        qml.AngleEmbedding(inputs, wires=range(n_qubits), rotation='Y')\n",
    "        qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "    weight_shapes = {\"weights\": (n_q_layers, n_qubits, 3)}\n",
    "    qnode = qml.QNode(qnode_circuit, dev, interface=\"torch\", diff_method=\"backprop\")\n",
    "    qlayer = TorchLayer(qnode, weight_shapes)\n",
    "    class QGBlock(nn.Module):\n",
    "        def __init__(self, reducer, qlayer, in_dim=512, out_dim=128, n_qubits=n_qubits):\n",
    "            super().__init__()\n",
    "            self.reducer = reducer\n",
    "            self.qlayer = qlayer\n",
    "            self.proj = nn.Linear(in_dim, n_qubits)\n",
    "            self.post = nn.Sequential(nn.Linear(n_qubits,out_dim), nn.ReLU(), nn.Dropout(0.3))\n",
    "        def forward(self, feat):\n",
    "            red = self.reducer(feat)\n",
    "            q_out = self.qlayer(red)\n",
    "            gates = torch.sigmoid(q_out)\n",
    "            proj_feat = self.proj(feat)\n",
    "            gated = proj_feat * gates\n",
    "            out = self.post(gated)\n",
    "            return out\n",
    "    class QGCNNHybrid(nn.Module):\n",
    "        def __init__(self, feat_ext, flatten, qg_block, num_classes):\n",
    "            super().__init__()\n",
    "            self.feat_ext = feat_ext; self.flatten = flatten; self.qg_block = qg_block\n",
    "            self.classifier = nn.Sequential(nn.Linear(128,64), nn.ReLU(), nn.Dropout(0.3), nn.Linear(64,num_classes))\n",
    "        def forward(self, x):\n",
    "            f = self.feat_ext(x); f = self.flatten(f)\n",
    "            out_qg = self.qg_block(f)\n",
    "            logits = self.classifier(out_qg)\n",
    "            return logits\n",
    "    qg_block = QGBlock(reducer, qlayer, in_dim=512, out_dim=128, n_qubits=n_qubits)\n",
    "    return QGCNNHybrid(feature_extractor, flatten, qg_block, n_classes)\n",
    "\n",
    "# ---- Evaluate a model: compute acc, f1, confusion, inference time per sample ----\n",
    "def evaluate_and_time(model, loader, device, n_warmup=10, n_samples=200):\n",
    "    model = model.to(device); model.eval()\n",
    "    preds=[]; trues=[]; losses=[]\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    # full eval metrics\n",
    "    with torch.no_grad():\n",
    "        for X,y in tqdm(loader, desc=\"Evaluation\", leave=False):\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            out = model(X)\n",
    "            loss = crit(out,y)\n",
    "            preds += torch.argmax(out, dim=1).cpu().tolist()\n",
    "            trues += y.cpu().tolist()\n",
    "            losses.append(loss.item())\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    f1 = f1_score(trues, preds, average='macro')\n",
    "    cm = confusion_matrix(trues, preds)\n",
    "    # inference time per sample (use a subset for timing)\n",
    "    all_inputs = []\n",
    "    for X,y in loader:\n",
    "        all_inputs.append(X)\n",
    "        if sum([t.shape[0] for t in all_inputs]) >= n_samples:\n",
    "            break\n",
    "    inputs_for_timing = torch.cat(all_inputs, dim=0)[:n_samples].to(device)\n",
    "    # warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_warmup):\n",
    "            _ = model(inputs_for_timing[:min(32, inputs_for_timing.shape[0])])\n",
    "    # timed runs\n",
    "    torch.cuda.synchronize() if device.type==\"cuda\" else None\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = model(inputs_for_timing)\n",
    "    torch.cuda.synchronize() if device.type==\"cuda\" else None\n",
    "    total_t = time.time() - t0\n",
    "    per_sample_ms = (total_t / inputs_for_timing.shape[0]) * 1000.0\n",
    "    return {\"acc\": acc, \"f1_macro\": f1, \"loss\": float(np.mean(losses)), \"cm\": cm.tolist(), \"latency_ms\": per_sample_ms}\n",
    "\n",
    "# ---- Load and evaluate models if checkpoints exist ----\n",
    "results = []\n",
    "\n",
    "# CNN baseline\n",
    "print(\"\\n--- Evaluating CNN baseline ---\")\n",
    "cnn_model = load_cnn_model(CKPT_CNN, DEVICE)\n",
    "cnn_params = param_count(cnn_model)\n",
    "res_cnn = evaluate_and_time(cnn_model, val_loader, DEVICE)\n",
    "res_cnn.update({\"model\":\"CNN_ResNet18\", \"params\": cnn_params})\n",
    "results.append(res_cnn)\n",
    "print(\"CNN done:\", {k:res_cnn[k] for k in ['acc','f1_macro','latency_ms','params']})\n",
    "\n",
    "# QCNN\n",
    "if CKPT_QCNN.exists():\n",
    "    print(\"\\n--- Evaluating QCNN hybrid ---\")\n",
    "    # use default qubit settings used for Phase4 (adjust if you changed)\n",
    "    qcnn_model = build_qcnn_model(n_qubits=6, n_q_layers=3)\n",
    "    qcnn_model = qcnn_model.to(DEVICE)\n",
    "    ck = torch.load(CKPT_QCNN, map_location=DEVICE)\n",
    "    # try load keys robustly\n",
    "    ms = ck.get(\"model_state\", ck.get(\"model_state_dict\", ck.get(\"model_state_dict\", None)))\n",
    "    try:\n",
    "        if ms is not None:\n",
    "            qcnn_model.load_state_dict(ms)\n",
    "        else:\n",
    "            qcnn_model.load_state_dict(ck)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: QCNN checkpoint load issue:\", e)\n",
    "    qcnn_params = param_count(qcnn_model)\n",
    "    res_qcnn = evaluate_and_time(qcnn_model, val_loader, DEVICE, n_samples=100)\n",
    "    res_qcnn.update({\"model\":\"QCNN_Hybrid\", \"params\": qcnn_params})\n",
    "    results.append(res_qcnn)\n",
    "    print(\"QCNN done:\", {k:res_qcnn[k] for k in ['acc','f1_macro','latency_ms','params']})\n",
    "else:\n",
    "    print(\"QCNN checkpoint missing; skipping QCNN eval.\")\n",
    "\n",
    "# QGCNN\n",
    "if CKPT_QGCNN.exists():\n",
    "    print(\"\\n--- Evaluating QGCNN hybrid ---\")\n",
    "    qg_model = build_qgcnn_model(n_qubits=4, n_q_layers=2)\n",
    "    qg_model = qg_model.to(DEVICE)\n",
    "    ck = torch.load(CKPT_QGCNN, map_location=DEVICE)\n",
    "    ms = ck.get(\"model_state\", ck.get(\"model_state_dict\", ck))\n",
    "    try:\n",
    "        qg_model.load_state_dict(ms)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: QGCNN checkpoint load issue:\", e)\n",
    "    qg_params = param_count(qg_model)\n",
    "    res_qg = evaluate_and_time(qg_model, val_loader, DEVICE, n_samples=100)\n",
    "    res_qg.update({\"model\":\"QGCNN_Hybrid\", \"params\": qg_params})\n",
    "    results.append(res_qg)\n",
    "    print(\"QGCNN done:\", {k:res_qg[k] for k in ['acc','f1_macro','latency_ms','params']})\n",
    "else:\n",
    "    print(\"QGCNN checkpoint missing; skipping QGCNN eval.\")\n",
    "\n",
    "# ---- Save numeric report ----\n",
    "df = pd.DataFrame(results)\n",
    "df = df[['model','params','acc','f1_macro','loss','latency_ms','cm']]\n",
    "df.to_csv(REPORT_CSV, index=False)\n",
    "print(\"\\nSaved CSV report ->\", REPORT_CSV)\n",
    "\n",
    "# ---- Plot comparison bar chart (accuracy, latency, params) ----\n",
    "plt.figure(figsize=(10,5))\n",
    "x = df['model']\n",
    "accs = df['acc']\n",
    "lat = df['latency_ms']\n",
    "params_m = df['params'] / 1e6\n",
    "\n",
    "ax1 = plt.subplot(1,3,1)\n",
    "ax1.bar(x, accs); ax1.set_title(\"Validation Accuracy\"); ax1.set_ylim(0,1)\n",
    "ax2 = plt.subplot(1,3,2)\n",
    "ax2.bar(x, lat); ax2.set_title(\"Inference latency (ms/sample)\")\n",
    "ax3 = plt.subplot(1,3,3)\n",
    "ax3.bar(x, params_m); ax3.set_title(\"Parameter count (M)\")\n",
    "ax3.set_ylabel(\"Millions\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_PNG, dpi=200)\n",
    "print(\"Saved comparison plot ->\", PLOT_PNG)\n",
    "\n",
    "# ---- Save confusion matrices and JSON ----\n",
    "out_json = {\"device\": str(DEVICE), \"results\": []}\n",
    "for r in results:\n",
    "    model_name = r.get(\"model\")\n",
    "    cm = np.array(r.get(\"cm\"))\n",
    "    # save cm figure\n",
    "    fig, ax = plt.subplots(figsize=(6,5))\n",
    "    im = ax.imshow(cm, cmap=\"Blues\")\n",
    "    ax.set_title(f\"Confusion matrix: {model_name}\")\n",
    "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    plt.savefig(f\"confusion_{model_name}.png\", dpi=150)\n",
    "    plt.close(fig)\n",
    "    # append textual results\n",
    "    out_json[\"results\"].append({k: (v if not hasattr(v, \"tolist\") else v) for k,v in r.items() if k!='cm'})\n",
    "    out_json[\"results\"][-1][\"cm_shape\"] = cm.shape\n",
    "\n",
    "with open(OUT_JSON, \"w\") as fh:\n",
    "    json.dump(out_json, fh, indent=2)\n",
    "print(\"Saved JSON report ->\", OUT_JSON)\n",
    "\n",
    "print(\"\\nPhase 6 complete. Files generated:\", REPORT_CSV, PLOT_PNG, OUT_JSON)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
